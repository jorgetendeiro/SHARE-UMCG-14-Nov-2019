---
title:    "Bayesian statistics as a therapy for frequentist problems"
subtitle: "Part 2: Bayes in action"
author:   "Jorge N. Tendeiro"
date:     "November 14, 2019<br><br> [https://rebrand.ly/Tendeiro_SHARE2](https://rebrand.ly/Tendeiro_SHARE2)"
header-includes:
  - \usepackage{amsmath}
output:   
  bookdown::html_document2:
    toc: true
    toc_float:
      collapsed: false
    toc_depth: 1
    fig_caption: true
    number_sections: true
    theme: cosmo        
    highlight: kate
    df_print: kable
    code_folding: show
  pdf_document:
    toc: yes
bibliography: ../references.bib
csl: ../style/apa-old-doi-prefix.csl
---

<img src="qrplot.png" width="250" height="250" />

<br><br>

# Basics

## Bayes rule

Say you have data $\mathcal{D}$, which you use to improve your understanding on the possible values of an unknown parameter $\theta$. For instance, $\theta$ can be the incidence rate of a particular medical condition in the population, and $\mathcal{D}$ the scores of a sample of patients on an indicator for that condition.

Bayesian statistics works as follows:

- Select a statistical model. The model conceptualizes how theory and data meet: It expresses, for any given value of the model parameter $\theta$, what the probability of *any* observed data is. This is denoted by $p(\mathcal{D}|\theta)$ (read: "probability of $\mathcal{D}$ conditional on a particular $\theta$ value") and often referred to as the *likelihood* function.

- *Before looking at the data*, express your belief about the value of $\theta$ in the population. This 'belief' does not equate with the 'religious' kind of belief. Instead, it refers to an educated expression of one's knowledge about the world. For instance, suppose the medical condition in your study is very rare: All research thus far suggests that less than 1% of the population is affected by it. This knowledge can (and *should*!) be incorporated in the analysis. The *prior* distribution, denoted $p(\theta)$, does exactly this.

- Collect your data.

- By means of the **Bayes rule** (see below), update your knowledge. Before the experiment, your knowledge about $\theta$ is encoded by the prior distribution. *After* looking at the data, you will rationally update your belief about $\theta$. The so-called *posterior* distribution, here denoted $p(\theta|\mathcal{D})$, represents your updated knowledge.

The Bayes rule can be written as follows:

$$
p(\theta|\mathcal{D}) = \frac{p(\theta)p(\mathcal{D}|\theta)}{p(\mathcal{D})}.
$$

It doesn't look nice...

Perhaps rewriting Bayes rule in words helps:

$$
\text{posterior} = \frac{\text{prior} \times \text{likelihood}}{\text{evidence}}.
$$

Notice that the evidence, $p(\mathcal{D})$, does not depend on $\theta$. It is there just to ensure that the posterior distribution is well defined (among other things that we won't be discussing today). For most purposes we can actually hide it and rewrite Bayes rule like this:

$$
\text{posterior} \propto \text{prior} \times \text{likelihood},
$$
where $\propto$ means "proportional to". 

Therefore, the posterior distribution is basically a (rational, logically correct) means of merging together both our prior knowledged about some phenomenon with the information about the phenomenon that our data has to offer.

## Summary

Bayesian statistics allows you to rationally update your knowledge about a phenomenon. All you need is three ingredients:

- a statistical model;
- a prior belief;
- data.

Any person with the same set of 3 ingredients will update his/her knowledge in the same way (that is what I mean by *rational* update). "The" unique way of updating our knowledge is given by the Bayes rule. This rule, by the way, is universal and accepted by frequentists and Bayesians alike (it is mathematically based on axioms). The Bayes rule is a mathematical necessity. Not accepting the Bayes rule is not accepting the basic axioms of probability.

# Set up

I will illustrate how to perform Bayesian estimation with a simple example. To make the contrast with the frequentist analysis clearer, I will fit the same model in the classic way too. This way, we can better compare the two paradigms. R will be the statistical software used.

The empirical data are a sample from the classical 'Intensive Care Unit' (ICU) data set [@hosmer2013; @lemeshow1988a]. This sample is freely available from the [vcdExtra](https://cran.r-project.org/web/packages/vcdExtra/index.html){target="_blank"} R package [@friendly2017a]. I will be fitting a logistic regression model which tries to predict the survival of adult patients following hospital discharge.

In order to fit Bayesian logistic regression, I decided to use the [rstanarm](https://cran.r-project.org/web/packages/rstanarm/index.html){target="_blank"} R package [@gabry2018]. This package offers a front-end to Stan ([https://mc-stan.org](https://mc-stan.org){target="_blank"}) and is relatively easy to use.

For the Bayesian analysis below, I drew inspiration from:

- A vignette from Aki Vehtari, Jonah Gabry, and Ben Goodrich ([https://avehtari.github.io/modelselection/diabetes.html](https://avehtari.github.io/modelselection/diabetes.html){target="_blank"}).

- The plotting tools of the [bayesplot](https://cran.r-project.org/web/packages/bayesplot/index.html){target="_blank"} package ([https://mc-stan.org/bayesplot/index.html](https://mc-stan.org/bayesplot/index.html){target="_blank"}).

- A blog post by Fabian Dablander ([https://fabiandablander.com/r/Law-of-Practice.html](https://fabiandablander.com/r/Law-of-Practice.html){target="_blank"}).

## Prepare environment

```{r results="hide", message=FALSE}
# Clean up:
rm(list = ls())
graphics.off()

# Load libraries (install if needed):
library(vcdExtra)       # For the ICU data
library(rstanarm)       # To fit Bayesian models
library(ggplot2)        # For plotting
library(pROC)           # ROC curve
library(loo)            # Leave-one-out CV
library(bridgesampling) # To compute BFs from rstanarm's output

# Parallel computing for Stan:
options(mc.cores = parallel::detectCores())
```

# The data

```{r message=FALSE}
# Load data (from the 'vcdExtra' package):
data(ICU)  # 200 cases, 22 variables

# Look at data:
  # DV = 'died'
  # See ?ICU for details on each variable.
str(ICU)   
```

# Fit logistic regression model

Inspired by the 'vcdExtra' example with these data, I decided to fit the following model for illustration purposes:

$$
\DeclareMathOperator{\logit}{logit}
\begin{align}
\logit(P(\text{died}=Yes))=\alpha + \beta_1\text{age} + \beta_2\text{cancer} + \beta_3\text{admit} + \beta_4\text{uncons}.
\end{align}
$$

Later on, to illustrate how model comparison can be performed under the Bayesian framework, I will also consider the reduced model which excludes predictor 'uncons':

$$
\DeclareMathOperator{\logit}{logit}
\begin{align}
\logit(P(\text{died}=Yes))=\alpha + \beta_1\text{age} + \beta_2\text{cancer} + \beta_3\text{admit}.
\end{align}
$$

## Frequentist

```{r}
icu.freq <- glm(died   ~ age + cancer + admit + uncons, 
                data   = ICU, 
                family = binomial)

summary(icu.freq)
```

Plot the estimated probability against the jittered outcome:
```{r message=FALSE, fig.height = 6}
est.prob <- predict(icu.freq, type = c("response"))
died.vec <- as.numeric(ICU$died) - 1 # 0 = No, 1 = Yes

plot(est.prob, jitter(died.vec, .2), 
     xlim = c(-.02, 1), ylim = c(-.08, 1.05), pch = 21, bg = "gray", bty = "n", 
     xlab = "", ylab = "", xaxs = "i", yaxs = "i", xaxt = "n", yaxt = "n")
# x-axis:
axis(1, at = c(-.02, 1), labels = c("", ""), lwd.ticks = 0)
axis(1, at = seq(0, 1, .2), lwd = 0, lwd.ticks = 1)
mtext("Estimated probability", 1, 2)
# y-axis:
axis(2, at = c(-.08, 1.05), labels = c("", ""), lwd.ticks = 0)
axis(2, at = seq(0, 1, .2), lwd = 0, lwd.ticks = 1, las = 1)
mtext("Jittered DV", 2, 2.5)
```

Plot sensitivity and specificity at all possible probability cutoff points:

```{r message=FALSE, fig.height = 6}
roc.out  <- roc(died ~ est.prob, data = ICU, ci = TRUE)

plot(roc.out$thresholds, roc.out$sensitivities, xlim = c(0, 1), 
     type = "l", lwd = 2, bty = "n", 
     xlab = "", ylab = "", axes = FALSE)
lines(roc.out$thresholds, roc.out$specificities, type = "l", lwd = 2, lty = 2)
# x-axis:
axis(1, pos = 0)
mtext("Probability cutoff", 1, 1.5)
# y-axis:
axis(2, pos = 0, las = 1)
mtext("Sensitivity / Specificity", 2, 1.5)
# 
abline(v = .2, col = 2, lwd = 2)
legend(.7, .8, c("Sensitivity" , "Specificity"), lty = 1:2, lwd = 2, seg.len = 3)
```

So a probability cutoff value of about .2 is the one that optimizes both sensitivity and specificity. For illustration purposes only, we will use this value in what follows.

Here are the histograms of the estimated probabilities by outcome value (died = No, died = Yes):

```{r message=FALSE, fig.height = 6}
par(mar = c(4, 6.5, 2.5, 1), xpd = NA)
layout(matrix(c(1, 2), ncol = 1))

cut      <- .1999 # basically .20
onemspec <- mean(est.prob[died.vec == 0] > cut)
sensit   <- mean(est.prob[died.vec == 1] > cut)

h1 <- hist(est.prob[died.vec == 0], plot = FALSE)
cutoff.bins <- cut(h1$breaks, c(-Inf, cut, Inf), right = TRUE)
plot(h1, col = c("gray", "red")[cutoff.bins], 
     las = 1, freq = FALSE, main = "", xlab = "")
title("Died = No", adj = 1)
arrows (cut+.02, 3, .40, 3, length = 0.08, lwd = 2, col = "red")
text(.5, 3, paste0("1 - Specificity = ", onemspec), adj = 0, col = "red")

hist(est.prob[died.vec == 1], freq = FALSE, las = 1, 
     col = c("gray", "red")[cutoff.bins], main = "", xlab = "")
title("Died = Yes", adj = 1)
arrows (cut+.02, 2.2, .40, 2.2, length = 0.08, lwd = 2, col = "red")
text(.5, 2.2, paste0("Sensitivity = ", sensit), adj = 0, col = "red")
mtext("Estimated probability", 1, 2)
segments(cut, par("usr")[1], cut, 7, lwd = 2, col = "red")
```

The two histograms look different enough: There is hope of building a good classifier. 

The ROC curve:

```{r message=FALSE, fig.height = 6}
# plot(roc.out)   # Not so nice!...

# A pimped-up ROC plot:
plot(1-roc.out$specificities, roc.out$sensitivities, 
     type = "l", lwd = 2, bty = "n", 
     xlab = "", ylab = "", axes = FALSE, 
     main = "AUC = .85")
segments(0, 0, 1, 1, lty = 2, col = "gray")
axis(1, pos = 0)
mtext("1 - Specificity", 1, 1.5)
axis(2, pos = 0, las = 1)
mtext("Sensitivity", 2, 1.5)
segments(onemspec, 0, onemspec, sensit, col = 2)
segments(0, sensit, onemspec, sensit, col = 2)
points(onemspec, sensit, pch = 21, bg = 2)
```

The AUR is .85, so the predicted probability of non survival after hospital discharge is much larger for the patients who actually did not make it. So the model is working as intended.

## Bayesian

The rstanarm command that we need looks virtually identical to the frequentist one:


```{r}
icu.bayes <- stan_glm(died   ~ age + cancer + admit + uncons, 
                      data   = ICU, 
                      family = binomial)

summary(icu.bayes, digits = 3)
```

***

**<span style="color:red">Important note:</span>** The function above relies on a *whole lot* of defaults (e.g., choice of priors, number of MCMC chains, burn-in period, etc. etc.). I decided to use all defaults to keep things simple: I want to focus on the end-product rather than on the model fitting intricacies. However, for more serious work, you are *strongly adviced* to think more carefully about the model fit and convergence.

For more information on all this, you can start by reading some of the vignettes and blog posts that I added above. Provided you understand what I show below, the rest that you need to learn becomes much more accessible.

***

Let's plot the posterior distributions:

```{r fig.height = 6}
plot(icu.bayes, "areas", prob = 0.95, prob_outer = 1) + geom_vline(xintercept = 0)
```

Humm, not very clear. <br>
Let's plot 'age' separately (its SE is way smaller than the others):

```{r fig.height = 6}
plot(icu.bayes, "areas", prob = 0.95, prob_outer = 1, 
     pars = c("(Intercept)", "cancerYes", "admitEmergency", "unconsYes")) + 
  geom_vline(xintercept = 0)

plot(icu.bayes, "areas", prob = 0.95, prob_outer = 1, pars = c("age")) + 
  geom_vline(xintercept = 0)
```


# Compare results

## Point estimates

```{r echo=FALSE}
df.coef <- round(data.frame(summary(icu.freq)$coef[, c("Estimate", "Std. Error")], 
                      summary(icu.bayes)[1:5, c("mean", "sd")]), 2)
colnames(df.coef) <- c("B freq", "SE freq",  "B bayes", "SE bayes")
df.coef
```

## Interval estimates

```{r eval=FALSE, message=FALSE, warning=FALSE, results='hide'}
confint           (icu.freq,  level = .95)  # Frequentist 95% **confidence** interval
posterior_interval(icu.bayes, prob  = .95)  # Bayesian 95% **credible** interval
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
freq.CIs  <- confint           (icu.freq,  level = .95)
bayes.CIs <- posterior_interval(icu.bayes, prob  = .95)
df.CIs <- round(data.frame(freq.CIs, bayes.CIs), 2)
colnames(df.CIs) <- c("LB freq", "UB freq",  "LB bayes", "UB bayes")
df.CIs
```


## Interim conclusions

The numerical estimates look very close... So, why should we bother using Bayesian statistics?

- First of all, the *interpretation* of the results is different. For instance, note that confidence interval $\not=$ credible interval (the latter is *much* more intuitive). 

- Bayesian statistics is very flexible (e.g., relatively easy to adapt model, priors, constraints).

- Also, Bayesian modeling allows really cool model assessment. Below we will highlight the following:

    - *Prior predictive distribution*: What type of data does your model of choice predict, *even before you look at your own data*? Is the model sensible to start with?
    
    - *Posterior predictive distribution*: How does data generated from the estimated model relate to your observed data? This also works for out-of-sample observed data.
    
    - *Model comparisons*: How to decide between competing models?

# Prior predictive distribution

What priors are being used by default in stan_glm()?<br>
(Remember: You should always strive to set up your own priors!)

```{r}
prior_summary(icu.bayes)
```

Humm, are these priors sensible? 

- You can plot them and judge.

- You can see what kind of data is generated from the model (priors and likelihood combined). This is what the *prior predictive distribution* is all about.

So, let's sample data from the prior predictive distribution. That is, we sample from the model *before* estimation (!).

```{r}
icu.bayes.priorPD <- stan_glm(died ~ age + cancer + admit + uncons, 
                              data   = ICU, 
                              family = binomial, 
                              prior_PD = TRUE, seed = 678) # draw from prior pred. dist.
```

As an example, let's see four sets of model-generated values of $P(\text{died}=Yes)$, based on the observed scores of the predictors (age, cancer, admit, uncons):

```{r fig.height = 6}
# Probabilities P(died = Yes):
priorPD.prob <- posterior_linpred(icu.bayes.priorPD, transform = TRUE, draws = 4)

par(mar = c(4, .5, 1, .5))
layout(matrix(1:4, nrow = 2))
for (i in 1:4)
{
  hist(priorPD.prob[i, ], xlim = c(0, 1), col = "gray", bty = "n", 
       xlab = "P(died = Yes)", ylab = "", yaxt = "n", main = "")
}
```

The model is rather flexible. Some flexibility is good, but too much may be ill devised.<br>
If needed, change the model (e.g., different likelihood, different priors).

We will come back to the prior predictive distribution later when performing models comparison through Bayes factors (Section \@ref(sec::BFs)).

# Posterior predictive distribution

How well do data generated from the *estimated* model relate to the observed data?

For instance, predicted outcome frequencies:

```{r fig.height = 6}
pp_check(icu.bayes, "ppc_hist", binwidth = .05)
```

Predicted outcome frequencies, a different view:

```{r fig.height = 6}
pp_check(icu.bayes, plotfun = "ppc_bars")
```

Overall $P(\text{died} = Yes)$:

```{r fig.height = 6}
pp_check(icu.bayes, plotfun = "ppc_stat", stat = "mean", binwidth = .01)
```

Prediction errors:

```{r fig.height = 6}
pp_check(icu.bayes, plotfun = "ppc_error_hist", nreps = 9, binwidth = .05)
```

Estimated logistic model across conditions, with 95% uncertainty bands:

```{r fig.height = 8}
age.vec    <- seq(min(ICU$age), max(ICU$age), length.out = 100)
design.mat <- matrix(c(1, 1, 1, 1, 1, 1, 1, 1,  # intercept
                       0, 0, 0, 0, 1, 1, 1, 1,  # cancer
                       0, 0, 1, 1, 0, 0, 1, 1,  # admit
                       0, 1, 0, 1, 0, 1, 0, 1), # uncons
                     ncol = 8, byrow = TRUE) 
post.mat   <- as.matrix(icu.bayes)

par(mar = c(3, 6.5, 2.5, 1))
layout(matrix(c(1, 2, 5, 3, 4, 5), nrow = 3), heights = c(3, 3, .8))
for (i in 1:4)
{
  plot(NULL, xlim = c(min(ICU$age), max(ICU$age)), ylim = c(0, 1), 
       las = 1, bty = "n", xaxs = "i", yaxs = "i", xaxt = "n", xlab = "", ylab = "")
  axis(1, at = c(min(ICU$age), max(ICU$age)), labels = c("", ""), lwd.ticks = 0)
  axis(1, at = seq(20, 80, 20), lwd = 0, lwd.ticks = 1)
  mtext("Age", 1, 2)
  
  for (j in 1:2)
  {
    design.coef  <- design.mat[, 2*(i-1) + j]
    mean.logis   <- plogis(sum(coef(icu.bayes)[-2] * design.coef) + coef(icu.bayes)[2] * age.vec)
    postPD.logis <- apply(post.mat, 1, function(vec) plogis(sum(vec[-2] * design.coef) + vec[2] * age.vec))
    L.025        <- apply(postPD.logis, 1, function(vec) quantile(vec, probs = .025))
    L.975        <- apply(postPD.logis, 1, function(vec) quantile(vec, probs = .975))
    
    points (age.vec, mean.logis, type = "l",  col = j+1)
    polygon(c(age.vec, rev(age.vec)), c(L.025, rev(L.975)), col = adjustcolor(j+1, .05), border = NA)
    
    if (i==1) mtext("cancer = No" , 3, .5, cex = 1.2)
    if (i==3) mtext("cancer = Yes", 3, .5, cex = 1.2)
    if (i==1) mtext("admit = Elective",  2, 3, cex = 1.2)
    if (i==2) mtext("admit = Emergency", 2, 3, cex = 1.2)
  }
}

par(mar = c(0, 0, 1, 0))
plot(0, type = "n", axes = FALSE, ann = FALSE, ylim = c(0, .1), xlim = c(0, 1))
legend("top", c("uncons = Yes", "uncons = No"), lty = 1, col = 3:2, cex = 1.5, 
       xjust = 0, yjust = .5, ncol = 2)
```

## Classification accuracy

Compare observed $y$ scores (0 = not died; 1 = died) with predicted $y$ scores.

First we do in-sample prediction:

```{r}
# Predicted probabilities:
postPD.probs <- posterior_linpred(icu.bayes, transform = TRUE)
postPD.prob  <- colMeans(postPD.probs)
postPD.pred  <- as.integer(postPD.prob >= 0.5)
   
# Posterior classification accuracy:
mean(died.vec == postPD.pred)
```

Classification accuracy can be inflated when estimated from seen data. It is better to use out-of-sample data, or at least cross-validation:

```{r}
# LOO predictive probabilities:
icu.bayes.loo <- loo(icu.bayes, save_psis = TRUE)
loo.prob      <- E_loo(postPD.probs, icu.bayes.loo$psis_object, 
                       type="mean", log_ratios = -log_lik(icu.bayes))$value

# LOO classification accuracy:
mean(died.vec == as.integer(loo.prob > .5))
```

So, we conclude that model-predicted probabilities and leave-one-out cross-validation probabilities match quite well:

```{r fig.height = 6}
plot(postPD.prob, loo.prob, xlim = c(0, 1), ylim = c(0, 1), 
     pch = 21, bg = "gray", bty = "n", las = 1, 
     xlab = "", ylab = "", xaxs = "i", yaxs = "i")
mtext("Posterior predictive probabilities", 1, 2)
mtext("LOO predictive probabilities", 2, 2.5)
abline(0, 1, lty = 2, col = "gray")
```

## Express uncertainity about pretty much anything!

Here is a motto that highlights one of the main benefits of Bayesian statistics:

> Express our uncertainty about any outcome by means of a probability distribution.

For instance, after fitting the logistic model the frequentist way, we found "the" ROC and "the" AUC. But these are just one instance of the ROC and the AUC, computed based on the frequentist estimates of the parameters of the logistic regression model. 

A Bayesian would ask: "How much uncertainty is there in the ROC and the AUC?"

The answer is simple: Use the posterior predictive distribution! Simply, compute the ROC and AUC for each replicated dataset generated from the posterior predictive distribution. This gives the posterior predictive distribution of the ROC and the AUC, which summarize all our uncertainty about them given our model and observed data.

```{r}
my.thresholds <- seq(0, 1, .01)
roc.bayes.sen <- matrix(NA, nrow(postPD.probs), length(my.thresholds))
roc.bayes.spe <- matrix(NA, nrow(postPD.probs), length(my.thresholds))
roc.bayes.auc <- rep   (NA, nrow(postPD.probs))

for (i in 1:nrow(postPD.probs)) 
{
  tmp                <- roc(ICU$died ~ postPD.probs[i, ], quiet = TRUE)
  tmp.std            <- coords(tmp, my.thresholds, transpose = FALSE, 
                               input = "thr", ret = c("sen", "spe"))
  roc.bayes.sen[i, ] <- tmp.std$sen
  roc.bayes.spe[i, ] <- tmp.std$spe
  roc.bayes.auc[i]   <- tmp$auc
}
```

Here are 4,000 ROC curves from the posterior predictive distribution:

```{r fig.height = 6}
plot(1 - roc.bayes.spe[1, ], roc.bayes.sen[1, ], 
     type = "l", col = "gray", bty = "n", 
     xlab = "", ylab = "", axes = FALSE)
for (i in 2:4000) points(1 - roc.bayes.spe[i, ], roc.bayes.sen[i, ], 
                         type = "l", col = "gray", lwd = .5)
points(1-roc.out$specificities, roc.out$sensitivities, 
       type = "l", lwd = 2)
segments(0, 0, 1, 1, lty = 2, col = "gray")
axis(1, pos = 0)
mtext("1 - Specificity", 1, 1.5)
axis(2, pos = 0, las = 1)
mtext("Sensitivity", 2, 1.5)
legend(.5, .3, c("ROC (Freq)" , paste0("ROC (Bayes, ", nrow(postPD.probs), c(" reps)"))), 
       lwd = c(2, .5), seg.len = 4, col = c("black", "gray"), cex = .8)
```

Humm, what are those strange ROCs on the bottom?<br>
After some inspection, I realized that those are related to negative estimates for the 'age' effect:

```{r fig.height = 6}
plot(1 - roc.bayes.spe[1, ], roc.bayes.sen[1, ], 
     type = "l", col = "gray", bty = "n", 
     xlab = "", ylab = "", axes = FALSE)
for (i in 2:4000) points(1 - roc.bayes.spe[i, ], roc.bayes.sen[i, ], 
                         type = "l", col = "gray", lwd = .5)
points(1-roc.out$specificities, roc.out$sensitivities, 
       type = "l", lwd = 2)
segments(0, 0, 1, 1, lty = 2, col = "gray")
axis(1, pos = 0)
mtext("1 - Specificity", 1, 1.5)
axis(2, pos = 0, las = 1)
mtext("Sensitivity", 2, 1.5)

for (i in which(as.matrix(icu.bayes)[, 2] < 0))
{
  points(1 - roc.bayes.spe[i, ], roc.bayes.sen[i, ], type = "l", col = "blue", lwd = 3)
}
legend(.5, .3, c("ROC (Freq)", 
                 paste0("ROC (Bayes, ", nrow(postPD.probs), c(" reps)")), 
                 "ROC (Bayes, age < 0)"), 
       lwd = c(2, .5, 3), seg.len = 4, col = c("black", "gray", "blue"), cex = .8)
```

So we learned something by inspecting the posterior predictive distribution. Perhaps we should consider refitting the model, this time adding a constraint for the age effect (would that be theoretically sensible?).

Here's the posterior distribution for the AUC:

```{r fig.height = 6}
hist(roc.bayes.auc, xlim = c(.6, .9), col = "gray", bty = "n", 
       xlab = "", ylab = "", yaxt = "n", 
     main = paste0("95% credible interval = (", 
                   round(quantile(roc.bayes.auc, .025), 2), 
                   ", ", 
                   round(quantile(roc.bayes.auc, .975), 2), 
                   ")"), 
     yaxs = "i")
mtext("AUC", 1, 2)
```

Finally, we noticed under the frequentist model that, at threshold 0.2, we had <br>
1-specificity = `r round(onemspec, 2)` <br>
and <br> 
sensitivity = `r round(sensit, 2)`. <br> 
How uncertain are we about these estimates?

```{r fig.height = 6}
th.pos <- which(my.thresholds == .20)

par(mar = c(4, 6.5, 2.5, 1))
layout(matrix(c(1, 2), ncol = 1))

hist(1 - roc.bayes.spe[, th.pos], freq = FALSE, las = 1, xlim = c(0, .7), 
     col = "gray", main = "", xlab = "")
mtext("1 - Specificity", 1, 2)
abline(v = c(onemspec, median(1 - roc.bayes.spe[, th.pos])), lty = 1:2, col = 2, lwd = 2)
legend(.42, 4, c(paste0("Freq. estimate = ", round(onemspec, 2)), 
                 paste0("Bayes (median) = ", round(median(1 - roc.bayes.spe[, th.pos]), 2))), 
       lwd = 2, lty = 1:2, seg.len = 2, col = 2, cex = .8)
legend(.42, 2, bty = "n", 
       paste0("95% credible int. = (", 
              round(quantile(1 - roc.bayes.spe[, th.pos], .025), 2), 
              ", ", 
              round(quantile(1 - roc.bayes.spe[, th.pos], .975), 2), 
              ")"), cex = .8)

hist(roc.bayes.sen[, th.pos], freq = FALSE, las = 1, 
     col = "gray", main = "", xlab = "")
mtext("Sensitivity", 1, 2)
abline(v = c(sensit, median(roc.bayes.sen[, th.pos])), lty = 1:2, col = 2, lwd = 2)
legend(.35, 5, c(paste0("Freq. estimate = ", round(sensit, 2)), 
                paste0("Bayes (median) = ", round(median( roc.bayes.sen[, th.pos]), 2))), 
       lwd = 2, lty = 1:2, seg.len = 2, col = 2, cex = .8)
legend(.35, 3, bty = "n", 
       paste0("95% credible int. = (", 
              round(quantile(roc.bayes.sen[, th.pos], .025), 2), 
              ", ", 
              round(quantile(roc.bayes.sen[, th.pos], .975), 2), 
              ")"), cex = .8)
```


# Model comparison

We can think of comparing competing models in two main ways (see [this](https://fabiandablander.com/r/Law-of-Practice.html#fnref:3){target="_blank"} blog post of Fabian Fablander for an enlightening discussion):

- *Posterior* prediction: How do the models fare against each other in terms of out-of-sample prediction error, based on the estimated model (i.e., posterior distributions)?

- *Prior* prediction: How do models fare against each other in terms of predicting the observed data (i.e., based on the prior distributions)?

Below we look at two posterior prediction tools (approximate leave-one-out cross-validation and k-fold cross-validation) and one prior prediction tool (the Bayes factor).

## (Approximate) Leave-one-out cross-validation

We can validate our model by conducting an approximate leave-one-out cross-validation [@vehtari2017]. For example, how much better does our model perform in comparison to a reduced model that excludes predictor 'uncons'? Here the idea is that we are looking for the model with better *posterior prediction* ability.

```{r}
icu.bayes.red     <- update(icu.bayes, died ~ age + cancer + admit)  # Fit reduced model
icu.bayes.red.loo <- loo(icu.bayes.red)
loo_compare(icu.bayes.red.loo, icu.bayes.loo)
```

We conclude that the full model is preferred.

For those used to the AIC, you can use the LOO equivalent (LOOIC):

```{r}
icu.bayes.loo
```

According to Gabry and Goodrich ([here](http://mc-stan.org/rstanarm/articles/rstanarm.html#step-2-draw-from-the-posterior-distribution){target="_blank"}),

> The "LOO Information Criterion (LOOIC)", has the same purpose as the Akaike Information Criterion (AIC) that is used by frequentists. Both are intended to estimate the expected log predicted density (ELPD) for a new dataset. However, the AIC ignores priors and assumes that the posterior distribution is multivariate normal, whereas the functions from the loo package used here do not assume that the posterior distribution is multivariate normal and integrate over uncertainty in the parameters.

## K-fold cross-validation

An alternative to leave-one-out cross-validation. <br>
(Be patient, it takes some time.)

```{r message=FALSE}
icu.bayes.kfold     <- rstanarm::kfold(icu.bayes,     K = 10)
icu.bayes.red.kfold <- rstanarm::kfold(icu.bayes.red, K = 10)
loo_compare(icu.bayes.red.kfold, icu.bayes.kfold)
```

Similarly, we have evidence supporting the full model.

## Bayes factors  {#sec::BFs} 

With LOO-CV and K-fold CV we compare models in terms of their posterior predictive qualities.

Bayes factors, on the other hand, focus on *prior prediction* qualities of the models under comparison:<br>
How well does each model (i.e., combination of likelihood and priors) predict the observed data?

```{r message=FALSE, warning=FALSE, results='hide'}
icu.bayes.bf     <- update(icu.bayes,     diagnostic_file = file.path(tempdir(), "df1.csv"))
icu.bayes.bridge <- bridge_sampler(icu.bayes.bf)

icu.bayes.red.bf     <- update(icu.bayes.red, diagnostic_file = file.path(tempdir(), "df2.csv"))
icu.bayes.red.bridge <- bridge_sampler(icu.bayes.red.bf)

bf(icu.bayes.bridge, icu.bayes.red.bridge)
```

There is overwhelming evidence in favor of the full model.

`r if (knitr::is_html_output()) '# References {-}'`